\documentclass[10pt]{beamer}
% \usepackage[utf8]{inputenc}
% \usepackage{xeCJK}
\usepackage{graphicx}
\usepackage {hyperref}
% \usepackage{utopia} %font utopia imported
\usetheme{Madrid}
\usecolortheme{default}

% set colors
\definecolor{myNewColorA}{RGB}{247, 128, 37}
\definecolor{myNewcolorA}{RGB}{247, 128, 37}
\definecolor{myNewcolorA}{RGB}{247, 128, 37}
\setbeamercolor{block title}{bg=myNewColorA,fg=black}
\setbeamercolor{block body}{bg=myNewColorA!20,fg=black}
\setbeamercolor{block title alerted}{bg=black, fg=myNewColorA}
\setbeamercolor{block body alerted}{bg=black!20, fg=black}

\setbeamercolor*{block title example}{bg=myNewColorA, fg = black}
\setbeamercolor*{block body example}{bg=myNewColorA!20, fg = black}
\usebeamercolor[myNewColorA]{block title alerted}
\setbeamercolor*{palette primary}{bg=myNewcolorA}
\setbeamercolor*{palette secondary}{bg=myNewcolorA, fg = white}
\setbeamercolor*{palette tertiary}{bg=myNewColorA, fg = white}
\setbeamercolor*{titlelike}{fg=myNewColorA}
\setbeamercolor*{title}{bg=myNewColorA}
\setbeamercolor*{item}{fg=myNewColorA}
\setbeamercolor*{caption name}{fg=myNewColorA}
\usefonttheme{professionalfonts}
\usepackage{natbib}
\usepackage{hyperref}
%------------------------------------------------------------
\titlegraphic{\includegraphics[height=1.8cm]{1gR_eRFZ_400x400}} 
\setbeamerfont{title}{size=\large}
\setbeamerfont{subtitle}{size=\small}
\setbeamerfont{author}{size=\small}
\setbeamerfont{date}{size=\small}
\setbeamerfont{institute}{size=\small}
\title[RGUKT Nuzvid]{\textbf{Gunshot Detection System On Edge Devices} }
% \subtitle{ Your Subtitle is Here}
\author[]{ Under the guidance of \textbf{Dr. SHAIK RIYAZ HUSSAIN SIR}\\Prepared by:\\
	\textbf{ P. Praveen Kumar - N210402 }}

\institute[]{RAJIV GANDHI UNIVERSITY OF KNOWLEDGE TECHNOGIES, NUZVID}
\date[\textcolor{white}{25 October 2025} ]
{ 25 October 2025}

%------------------------------------------------------------
%This block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:
%\AtBeginSection[]
%{
	%  \begin{frame}
		%    \frametitle{Contents}
		%    \tableofcontents[currentsection]
		%  \end{frame}
	%} 
\AtBeginSection[]{ 
	\begin{frame} 
		\vfill 
		\centering 
		\begin{beamercolorbox}[sep= 8pt,center,shadow=true,rounded=true]{title} \usebeamerfont{title}\insertsectionhead\par
		\end{beamercolorbox} \vfill \end{frame} }

%------------------------------------------------------------
\begin{document}
	%The next statement creates the title page.
	\frame{\titlepage}
	\begin{frame}
		\frametitle{Table of Contents}
		\tableofcontents	
	\end{frame}
	
	%------------------------------------------------------------
	\section{Abstract}
	\begin{frame}{Abstract}
		\frametitle{Abstract}
		\begin{itemize}
			\item Gunshot detection plays a vital role in enhancing public safety and
			real-time surveillance systems. Detecting firearm sounds accurately is
			essential for enabling rapid response and preventing potential hazards
			in urban and critical infrastructure environments. Early approaches
			used traditional machine learning techniques like SVM, Random Forest,
			and k-NN, but these methods struggled with noisy data and limited
			generalization to real-world audio conditions.
			
			\item Our research focuses on developing an efficient deep learning-based
			gunshot detection framework using embedded acoustic feature
			representations. We preprocess raw audio into mel-spectrograms and
			generate compact embeddings that capture key temporalâ€“spectral
			patterns. A BiLSTM-Attention architecture is employed to model both
			sequential and contextual audio dynamics effectively, achieving robust
			classification between gunshot and non-gunshot events.
			
			\item To further enhance the modelâ€™s real-time performance, we implemented
			optimization techniques including dynamic quantization and pruning.
			These methods reduced computational overhead and latency without
			significant accuracy loss, achieving a detection accuracy of 96.7%.
			
			
		\end{itemize}
	\end{frame}
	
	
	\section{Introduction}
	\begin{frame}{Introduction}
		\frametitle{Introduction}
	\section*{Introduction}
	
	\begin{itemize}
		
		\item Gunshot detection systems have become increasingly important in
		modern intelligent surveillance and public safety applications. With the
		rise in firearm-related incidents and the need for rapid response,
		real-time acoustic event detection can significantly improve situational
		awareness in urban and critical infrastructure environments.
		
		\item Traditional gunshot detection methods relied on handcrafted features
		and classical machine learning algorithms such as Support Vector
		Machines (SVM), k-Nearest Neighbors (kNN), and Random Forests.
		Although these approaches achieved reasonable results under
		controlled conditions, their performance often degraded in noisy and
		dynamic real-world environments due to their limited capacity to
		capture complex temporal and spectral audio variations.
		
		\item To overcome these challenges, recent advances in deep learning have
		enabled the automatic extraction of hierarchical and discriminative
		audio features. Convolutional Neural Networks (CNNs) and Recurrent
		Neural Networks (RNNs) have been widely explored for audio event
		classification tasks, offering improved robustness and adaptability.
		
		
	\end{itemize}
	
	\end{frame}
	\begin{frame}{Introduction}
		\begin{itemize}
		
		\item In this work, we present a deep learning-based gunshot detection
		framework that combines temporal modeling and contextual attention
		mechanisms. The proposed system leverages embedded acoustic
		feature representations processed through a BiLSTM-Attention
		architecture. These embeddings capture the essential frequencyâ€“
		time relationships while maintaining computational efficiency.
		
		\item The model was further optimized using pruning and quantization
		techniques to reduce latency and resource consumption without
		compromising accuracy. This optimization enables efficient deployment
		on embedded and real-time surveillance platforms, making the proposed
		approach both lightweight and reliable for real-world use.
		\end{itemize}
		
	\end{frame}
	%-----------------------------------------
	\section{Base Paper Overview}
	%-----------------------------------------
\begin{frame}{Base Paper Overview}
	\frametitle{Real-time Gunshot Detection System}
	\justifying
	The base paper presents an autonomous gunshot detection system to enhance public safety. 
	The system continuously captures environmental audio and processes it for detecting gunshot events in real-time. 
	The following key aspects summarize the methodology and observations:
	\begin{itemize}
		\item Microphones capture audio streams which are processed using MFCC features or pre-trained embeddings like YAMNet.
		\item Neural network classifiers are employed for detecting gunshots, achieving high accuracy (96\%).
		\item Designed for deployment on low-power edge devices, including Raspberry Pi, for real-time operation.
	\end{itemize}
\end{frame}
%----------------------------------
\begin{frame}{Key Contributions and Limitations of Base Paper}
	\frametitle{Analysis of Base Paper}
	\justifying
	The base paper made significant contributions to real-time gunshot detection but also had limitations that inspired our enhancements:
	\begin{itemize}
		\item \textbf{Contributions:}
		\begin{itemize}
			\item Evaluation of multiple hardware platforms for real-time inference feasibility.
			\item Consideration of noisy environments and confounding audio events.
		\end{itemize}
		\item \textbf{Limitations:}
		\begin{itemize}
			\item Hardware optimization and low-latency inference were limited.
			\item Model compression techniques like pruning or quantization were not explored.
			\item Lightweight embedding-only inference for resource-constrained devices was not implemented.
			\item Model was trained on small dataset only leading to  overfitttig.
		\end{itemize}
	\end{itemize}
\end{frame}
%------------------------------------------------------
\begin{frame}{Base Paper Implementation}
	\frametitle{Gunshot Detection System Overview}
	\justifying
	The base paper focuses on the design and implementation of a real-time gunshot detection system integrated into a camera surveillance system. 
	The audio dataset consists of gunshot and non-gunshot sounds, pre-processed to enhance model performance.
	
	\begin{itemize}
		\item Gunshot data included various firearms such as AK-47, MP5, MG-42, M16, M4, IMI Desert Eagle, AK12, and Zastava M92. 
		The audio was resampled to 1-second 22050 Hz arrays, and a power threshold filter was applied to remove low-energy or irrelevant samples. 
		After filtering, 3210 gunshot samples remained for model training.
		\item Non-gunshot audio was collected from YouTube and included thunder, snaps, fireworks, drums, doors, clapping, and barks. 
		After preprocessing, 7758 non-gunshot samples were available. No power threshold was applied since minimal energy is considered non-gunshot.
	\end{itemize}
\end{frame}

\begin{frame}{Base Paper Implementation (Model Training)}
	\frametitle{Modeling Approaches}
	\justifying
	Two distinct machine learning approaches were implemented:
	\begin{enumerate}
		\item \textbf{YAMNet Transfer Learning:} 
		\begin{itemize}
			\item Pre-trained YAMNet embeddings (1024-dim) were used as input.
			\item A three-layered sequential network was trained with 512-unit dense layer and an output layer of 2 neurons (gunshot/non-gunshot).
			\item Dataset split: 60\% training, 20\% validation, 20\% testing; total of 3200 samples per group.
			\item EarlyStopping callback used to prevent overfitting; model trained for 5 epochs.
		\end{itemize}
		
		\item \textbf{MFCC-Based LSTM Approach:} 
		\begin{itemize}
			\item Audio converted to Mel-Frequency Cepstral Coefficients (MFCCs) using Librosa.
			\item LSTM model architecture: 128-unit LSTM layer, Flatten, Dense layers with 128 and 64 units, three Dropout layers, output with 9 neurons for multi-class classification.
			\item Training over 50 epochs with batch size 72, using SparseCategoricalCrossentropy loss and EarlyStopping.
		\end{itemize}
	\end{enumerate}
\end{frame}

	%-----------------------------------------------
\begin{frame}{Training Accuracy and Loss Graphs}
	\frametitle{Model Performance Overview}
	\justifying
	The training and validation performance of the two gunshot detection models is shown below. 
	Both approaches achieved high accuracy on the training, validation, and test datasets.
		\vspace{0.3cm}
	\begin{columns}
		\column{0.48\textwidth}
		\centering
		\textbf{YAMNet Transfer Learning} \\
		Accuracy: 98.52\% \\
		\includegraphics[width=\textwidth]{Yamnet_Accuracy.png}
		
		\column{0.48\textwidth}
		\centering
		\textbf{MFCC + LSTM Model} \\
		Accuracy: 96.95\% \\
		\includegraphics[width=\textwidth]{accuracy_curve_MFCC.png}
	\end{columns}
	


\end{frame}
%--------------------------------------------
	\section{Model Advancements}
	%-----------------------------------------------
\begin{frame}{Proposed Advanced Model â€” CNN14 + BiLSTM + Attention}
	\justifying
	\begin{columns}[T,onlytextwidth]
		
		% ===== Left Column =====
		\begin{column}{0.58\textwidth}
			\vspace{-0.2cm}
			\textbf{Goal:} Improve gunshot detection by combining spectralâ€“temporal learning and attention focus.
			
			\vspace{0.2cm}
			\textbf{Model Highlights:}
			\begin{itemize}
				\item \textbf{CNN14 (PANNs):} Extracts deep log-Mel embeddings from preprocessed audio.
				\item \textbf{BiLSTM:} Learns forwardâ€“backward temporal context of gunshot events.
				\item \textbf{Attention:} Weighs key frames, suppresses noise/silence.
				\item \textbf{Dense Head:} 128â€“64 ReLU units + dropout.
				\item \textbf{Output:} Softmax â†’ \textit{Gunshot / Non-Gunshot}.
			\end{itemize}
		\end{column}
		
		% ===== Right Column =====
		\begin{column}{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth,height=4.6cm,keepaspectratio]{architecture_model.png}\\
			{\scriptsize CNN14 â†’ BiLSTM â†’ Attention â†’ Dense â†’ Output}
		\end{column}
		
	\end{columns}
\end{frame}




	%----------------------------------------------------------
\begin{frame}{Dataset Preparation}
	\justifying
	To develop a robust and generalized gunshot detection system, audio samples were collected and organized from multiple open-source repositories.  
	The datasets were categorized into two primary classes â€” \textbf{Gunshot} and \textbf{Non-Gunshot} â€” to ensure balanced binary classification.
	
	\vspace{0.3cm}
	\textbf{Datasets Used:}
	
	\begin{columns}[T,onlytextwidth]
		\begin{column}{0.48\textwidth}
			\textbf{Gunshot Audio Sources:}
			\begin{itemize}
				\item Gunshot Audio Dataset (Kaggle)
				\item Mendeley Gunshot Dataset
				\item Gunshot/Gunfire Dataset (Zenodo â€“ Edge Collected)
				\item MAD â€“ Military Audio Dataset
			\end{itemize}
		\end{column}
		
		\begin{column}{0.48\textwidth}
			\textbf{Non-Gunshot Audio Sources:}
			\begin{itemize}
				\item UrbanSound8K Dataset
				\item ESC-50 Environmental Sound Dataset
			\end{itemize}
		\end{column}
	\end{columns}
	
	\vspace{0.3cm}
	\textbf{Dataset Composition:}
	\begin{itemize}
		\item Total of \textbf{17,746 audio clips}, with \textbf{8,873 samples per class}.
		\item Ensured balanced representation of diverse real-world gunshot and background sounds.
	\end{itemize}
\end{frame}

%-----------------------------------------------
\begin{frame}{Data Preprocessing Pipeline}
	\begin{columns}[T,onlytextwidth]
		
		% ========== LEFT COLUMN ==========
		\begin{column}{0.52\textwidth}
			\textbf{1. Raw Data Filtering}
			\begin{itemize}
				\item Loaded all audio clips at \textbf{16 kHz, mono}.
				\item Applied \textbf{RMS power filtering} to remove silent or low-energy files:
				\begin{itemize}
					\item Threshold: \textbf{RMS} $\geq$ 0.002
					\item At least one RMS peak (\textbf{min\_peak\_count} = 1)
				\end{itemize}
				\item Result: kept \textbf{5,773 / 8,883} valid gunshot clips.
			\end{itemize}
			
			\vspace{0.1cm}
			\textbf{2. Balancing and Sliding Windows}
			\begin{itemize}
				\item Balanced dataset by selecting \textbf{5,773 non-gunshot} samples
				from UrbanSound8K \& ESC-50 categories.
				\item Generated fixed-length \textbf{sliding windows (â‰ˆ1s)} from both classes.
			\end{itemize}
		\end{column}
		
		% ========== RIGHT COLUMN ==========
		\begin{column}{0.46\textwidth}
			\textbf{3. Feature Extraction (Librosa)}
			\begin{itemize}
				\item Sampling rate: \textbf{32 kHz}
				\item Computed \textbf{log-Mel spectrograms}:
				\begin{itemize}
					\item 64 Mel bins, 10 ms hop, 50â€“14 kHz band
					\item Per-frequency normalization (zero mean, unit variance)
				\end{itemize}
			\end{itemize}
			
			\vspace{0.15cm}
			\centering
			\includegraphics[width=4cm,height=4cm,keepaspectratio]{Pre_processing_Adv.png}
		\end{column}
		
	\end{columns}
\end{frame}
%=---------------------------------------------
\begin{frame}{Final Model â€” CNN14 Embedding Classifier}
	\justifying
	A lightweight \textbf{CNN14 embedding-based classifier} designed for real-time gunshot detection. Redundant audio layers removed while preserving key learned representations.
	
	\vspace{0.2cm}
	\textbf{Architecture:}
	\begin{itemize}
		\item \textbf{Input:} Log-Mel spectrogram (1Ã—64Ã—400), 4 s audio.
		\item \textbf{Backbone:} CNN14 encoder (PANNs) up to global avg pooling.
		\item \textbf{Embedding:} 2048-D feature vector.
		\item \textbf{Head:} Dense(512, ReLU) â†’ Dropout(0.3) â†’ Dense(1, Sigmoid).
	\end{itemize}
	
	\vspace{0.15cm}
	\textbf{Training:}
	\begin{itemize}
		\item Loss: Binary Crossentropy, Optimizer: Adam (lr = 1e-4).
		\item Metrics: Accuracy, AUC; 50 epochs.
		\item Backbone frozen, then fine-tuned.
	\end{itemize}
	
	\vspace{0.2cm}
	\begin{columns}[T,onlytextwidth]

		
		\begin{column}{0.45\textwidth}
			\small
			\textbf{Highlights:}
			\begin{itemize}
				\item Embedding-only â†’ fewer ops & memory.
				\item Deployable on \textbf{Raspberry Pi}.
				\item High accuracy, low latency.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

	%-----------------------------------------------
	
	\section{Result}}
	%-----------------------------------------------
\begin{frame}{Model Evaluation Overview \& Real-Time Superiority}
	\justifying
	\textbf{Evaluation Summary:}
	\begin{itemize}
		\item Developed and trained a CNN14 + BiLSTM + Attention model on 4s log-Mel spectrogram inputs.
		\item Dataset split: 60\% Training, 20\% Validation, 20\% Testing.
		\item Optimizer: \textbf{Adam (lr = 1e-4)}; Loss: \textbf{CrossEntropyLoss}; Total epochs: 50.
	\end{itemize}
	
	\vspace{0.25cm}
	\textbf{Why This Model Excels in Real-Time:}
	\begin{itemize}
		\item \textbf{Embedding-only Design:} Uses pre-trained CNN14 embeddings, eliminating redundant convolutional layers for faster computation.
		\item \textbf{Lightweight Inference:} Compact structure with minimal parameters enables deployment on low-power devices like \textbf{Raspberry Pi}.
		\item \textbf{BiLSTM-Attention Mechanism:} Learns key temporal and spectral cues, allowing rapid detection of impulsive sounds amidst noise.
		\item \textbf{Optimized Trade-off:} Achieves high accuracy (\textasciitilde97.6\%) with reduced latency and memory footprint.
		\item \textbf{Edge-Ready Deployment:} Ensures stable performance in real-world, noisy outdoor conditions.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Outcome:} A robust and efficient gunshot detection framework achieving near real-time response without compromising precision.
\end{frame}

	%----------------------------------------------
\begin{frame}{Performance Results and Analysis}
	\justifying
	\textbf{Training Summary:}
	\begin{itemize}
		\item \textbf{Epochs:} 50 \hspace{0.5cm} \textbf{Optimizer:} Adam (lr = 0.001)
		\item \textbf{Loss Function:} CrossEntropyLoss()
		\item \textbf{Best Validation Accuracy:} \textbf{97.57\%}
	\end{itemize}
	
	\vspace{0.15cm}
	\textbf{Key Observations:}
	\begin{itemize}
		\item Rapid convergence with minimal overfitting (EarlyStopping + Dropout).
		\item Attention layer improves focus on gunshot frames, enhancing noise resilience.
		\item Stable performance across urban and open-field tests.
	\end{itemize}
	
	\vspace{0.2cm}
	\begin{columns}[T,onlytextwidth]
		\begin{column}{0.48\textwidth}
			\centering
			\includegraphics[width=\linewidth,height=3.2cm,keepaspectratio]{acc vs loss_adv.png}\\
			{\scriptsize Accuracy vs Loss Accuracy}
		\end{column}
		\begin{column}{0.48\textwidth}
			\centering
			\includegraphics[width=\linewidth,height=3.2cm,keepaspectratio]{confusion_matrix.png}\\
			{\scriptsize Confusion Matrix (Gunshot vs Non-Gunshot)}
		\end{column}
	\end{columns}
	
	\vspace{0.15cm}
	\centering
\end{frame}
%-------------------------------------
\section{Implementation}
\begin{frame}{Raspberry Pi Implementation â€” System Setup}
	\justifying
	\textbf{Hardware Setup:}
	\begin{itemize}
		\item \textbf{Raspberry Pi 4} (4GB RAM) for edge deployment.
		\item USB microphone or external audio input for real-time audio capture.
		\item Optional: small speaker/alert system for local notifications.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Software Stack:}
	\begin{itemize}
		\item Python 3.x environment with \textbf{Librosa, Numpy, Torch}.
		\item Pre-trained \textbf{CNN14 embedding extractor} and BiLSTM-Attention classifier converted for lightweight edge inference.
		\item Optimized audio pipeline:
		\begin{itemize}
			\item Capture 1-second audio segments.
			\item Convert to log-Mel spectrogram.
			\item Pass through embedding extractor â†’ BiLSTM-Attention classifier.
		\end{itemize}
	\end{itemize}

	\textbf{Workflow Overview:}
	\begin{center}
		\includegraphics[width=0.85\linewidth,height=3cm,keepaspectratio]{Raspberry Pi Workflow.png} \\
		{\scriptsize Audio Capture â†’ Preprocessing â†’ Embedding â†’ Classification â†’ Alert}
	\end{center}
\end{frame}
%------------------------------------------------
\begin{frame}{Raspberry Pi Implementation â€” Real-Time Inference}
	\justifying
	\textbf{Inference Pipeline:}
	\begin{itemize}
		\item Continuous audio stream segmented into 1-second windows.
		\item Preprocessing and log-Mel extraction applied in real-time.
		\item CNN14 embedding extracted, followed by BiLSTM-Attention classification.
		\item Softmax output triggers \textbf{real-time alerts} for gunshot detection.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Optimizations for Edge Deployment:}
	\begin{itemize}
		\item Reduced model size using \textbf{embedding-only design}.
		\item Minimal CPU/memory usage for stable performance on Raspberry Pi.
		\item Low-latency inference (~100â€“200ms per segment).
		\item Robust to environmental noise, tested in urban and open-field scenarios.
	\end{itemize}
	
	\vspace{0.2cm}
	\centering
	\textbf{Outcome:}  
	\textit{A fully functional, low-latency, real-time gunshot detection system deployable on edge devices.}
\end{frame}


%--------------------------------
	\section{Comparison With Base Paper}}

	%-------------------------------------------
\begin{frame}{Baseline vs Proposed Model â€” Conceptual Comparison}
	\justifying
	\begin{columns}[T,onlytextwidth]
		% Baseline Paper
		\begin{column}{0.48\textwidth}
			\textbf{Baseline (MFCC + LSTM Paper):}
			\begin{itemize}
				\item Audio converted to MFCCs (FFT: 512, Hop: 255)
				\item Single LSTM (128 units) + Flatten + Dense(128,64) + Dropout
				\item Output: Softmax (9 classes in original; binary subset used here)
				\item Preprocessing: Resampled 1s, 22050 Hz, sliding-window 2000 Hz
				\item Gunshot power filter applied; non-gunshot not thresholded
				\item Training: SparseCategoricalCrossentropy, Adam, 50 epochs, batch 72
			\end{itemize}
			\centering

		\end{column}
		
		% Proposed Model
		\begin{column}{0.48\textwidth}
			\textbf{Proposed Model (CNN14 + BiLSTM + Attention):}
			\begin{itemize}
				\item Embedding-only CNN14 extracts high-level log-Mel features
				\item BiLSTM captures bidirectional temporal patterns
				\item Attention highlights key frames, suppressing silent/noisy regions
				\item Dense layers (512 â†’ 1) with Dropout, output: Sigmoid (binary)
				\item Preprocessing: Sliding-window 1s segments, RMS power filter, balanced classes
				\item Optimized for edge devices; faster inference & low memory
			\end{itemize}
			\centering

		\end{column}
	\end{columns}
\end{frame}

	%--------------------------------------------
\begin{frame}{Performance Comparison â€” Baseline vs Proposed}
	\justifying
	\centering
	\small
	\begin{tabular}{l c c}
		\hline
		\textbf{Metric} & \textbf{Baseline} & \textbf{Proposed} \\
		\hline
		Gunshot Samples & 3,210 & 5,773 \\
		Non-Gunshot Samples & 3,600 & 5,773 \\
		Input & MFCC (128Ã—N) & Log-Mel (64Ã—400) \\
		Temporal Model & LSTM & BiLSTM + Attention \\
		Validation Accuracy & ~62\% & 97.57\% \\
		Inference Latency & High & Low (Edge-ready) \\
		Model Size & Medium & Compact \\
		Noise Robustness & Moderate & High \\
		Deployment & Raspberry Pi & Raspberry Pi \\
		\hline
	\end{tabular}
	
	\vspace{0.3cm}
	\centering
	{\scriptsize Proposed model improves accuracy, reduces latency, and is robust for real-time edge deployment.}
	
	\vspace{0.2cm}
	\begin{columns}[c,onlytextwidth]
		\begin{column}{0.33\textwidth}
			\centering
			\textbf{âœ… Higher Accuracy} \\
			97.57\% validation
		\end{column}
		\begin{column}{0.33\textwidth}
			\centering
			\textbf{âš¡ Low Latency} \\
			Edge-ready for Raspberry Pi
		\end{column}
		\begin{column}{0.33\textwidth}
			\centering
			\textbf{ðŸŽ§ Robust} \\
			Performs well in noisy conditions
		\end{column}
	\end{columns}
\end{frame}
%-------------------------------------------------
\begin{frame}{Conclusion â€” Methodology}
	\justifying
	\textbf{Dataset and Preprocessing:}
	\begin{itemize}
		\item Curated a balanced dataset of \textbf{17,746 audio clips} (8,873 gunshot, 8,873 non-gunshot).
		\item Applied \textbf{RMS power filtering} to remove silent/irrelevant clips.
		\item Converted all audio to \textbf{1-second mono segments} and applied \textbf{sliding-window segmentation}.
		\item Extracted \textbf{log-Mel spectrograms} and embeddings from pre-trained CNN14 for transfer learning.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Model Development:}
	\begin{itemize}
		\item Built an \textbf{embedding-only CNN14 + BiLSTM + Attention} model for gunshot detection.
		\item Dense layers with dropout ensure generalization; output layer predicts binary classes (\textit{Gunshot / Non-Gunshot}).
		\item Optimized for \textbf{real-time inference} on Raspberry Pi / FPGA.
	\end{itemize}
\end{frame}
%-------------------------
\begin{frame}{Conclusion â€” Results and Key Takeaways}
	\justifying
	\textbf{Performance Highlights:}
	\begin{itemize}
		\item Achieved \textbf{97.57\% validation accuracy}, outperforming baseline MFCC + LSTM (~96\%).
		\item Low inference latency and compact model size for edge deployment.
		\item Attention mechanism improves focus on key frames and robustness to noise.
		\item Balanced training dataset ensures consistent performance across environments.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Key Takeaway:}
	\begin{itemize}
		\item Our approach demonstrates a \textbf{robust, real-time, and accurate gunshot detection system}.
		\item Outperforms classical MFCC + LSTM methods in both accuracy and edge deployment feasibility.
		\item Complete end-to-end workflow: preprocessing â†’ embedding â†’ model training â†’ evaluation â†’ real-time inference.
	\end{itemize}
\end{frame}


	%-----------------------------------------------

	
	\section*{Acknowledgement}  
	\begin{frame}
		\textcolor{myNewColorA}{\Huge{\centerline{Thank you!}}}
	\end{frame}
	
\end{document}