\documentclass[10pt]{beamer}
% \usepackage[utf8]{inputenc}
% \usepackage{xeCJK}
\usepackage{graphicx}
\usepackage {hyperref}
% \usepackage{utopia} %font utopia imported
\usetheme{Madrid}
\usecolortheme{default}

% set colors
\definecolor{myNewColorA}{RGB}{247, 128, 37}
\definecolor{myNewcolorA}{RGB}{247, 128, 37}
\definecolor{myNewcolorA}{RGB}{247, 128, 37}
\setbeamercolor{block title}{bg=myNewColorA,fg=black}
\setbeamercolor{block body}{bg=myNewColorA!20,fg=black}
\setbeamercolor{block title alerted}{bg=black, fg=myNewColorA}
\setbeamercolor{block body alerted}{bg=black!20, fg=black}

\setbeamercolor*{block title example}{bg=myNewColorA, fg = black}
\setbeamercolor*{block body example}{bg=myNewColorA!20, fg = black}
\usebeamercolor[myNewColorA]{block title alerted}
\setbeamercolor*{palette primary}{bg=myNewcolorA}
\setbeamercolor*{palette secondary}{bg=myNewcolorA, fg = white}
\setbeamercolor*{palette tertiary}{bg=myNewColorA, fg = white}
\setbeamercolor*{titlelike}{fg=myNewColorA}
\setbeamercolor*{title}{bg=myNewColorA}
\setbeamercolor*{item}{fg=myNewColorA}
\setbeamercolor*{caption name}{fg=myNewColorA}
\usefonttheme{professionalfonts}
\usepackage{natbib}
\usepackage{hyperref}
%------------------------------------------------------------
\titlegraphic{\includegraphics[height=1.8cm]{1gR_eRFZ_400x400}} 
\setbeamerfont{title}{size=\large}
\setbeamerfont{subtitle}{size=\small}
\setbeamerfont{author}{size=\small}
\setbeamerfont{date}{size=\small}
\setbeamerfont{institute}{size=\small}
\title[RGUKT Nuzvid]{\textbf{Gunshot Detection System On Edge Devices} }
% \subtitle{ Your Subtitle is Here}
\author[]{ Under the guidance of \textbf{Dr. SHAIK RIYAZ HUSSAIN SIR}\\Prepared by:\\
	\textbf{ P. Praveen Kumar - N210402 }}

\institute[]{RAJIV GANDHI UNIVERSITY OF KNOWLEDGE TECHNOGIES, NUZVID}
\date[\textcolor{white}{25 October 2025} ]
{ 25 October 2025}

%------------------------------------------------------------
%This block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:
%\AtBeginSection[]
%{
	%  \begin{frame}
		%    \frametitle{Contents}
		%    \tableofcontents[currentsection]
		%  \end{frame}
	%} 
\AtBeginSection[]{ 
	\begin{frame} 
		\vfill 
		\centering 
		\begin{beamercolorbox}[sep= 8pt,center,shadow=true,rounded=true]{title} \usebeamerfont{title}\insertsectionhead\par
		\end{beamercolorbox} \vfill \end{frame} }

%------------------------------------------------------------
\begin{document}
	%The next statement creates the title page.
	\frame{\titlepage}
	\begin{frame}
		\frametitle{Table of Contents}
		\tableofcontents	
	\end{frame}
	
	%------------------------------------------------------------
	\section{Abstract}
	\begin{frame}{Abstract}
		\frametitle{Abstract}
		\begin{itemize}
			\item Gunshot detection plays a vital role in enhancing public safety and
			real-time surveillance systems. Detecting firearm sounds accurately is
			essential for enabling rapid response and preventing potential hazards
			in urban and critical infrastructure environments. Early approaches
			used traditional machine learning techniques like SVM, Random Forest,
			and k-NN, but these methods struggled with noisy data and limited
			generalization to real-world audio conditions.
			
			\item Our research focuses on developing an efficient deep learning-based
			gunshot detection framework using embedded acoustic feature
			representations. We preprocess raw audio into mel-spectrograms and
			generate compact embeddings that capture key temporal‚Äìspectral
			patterns. A BiLSTM-Attention architecture is employed to model both
			sequential and contextual audio dynamics effectively, achieving robust
			classification between gunshot and non-gunshot events.
			
			\item To further enhance the model‚Äôs real-time performance, we implemented
			optimization techniques including dynamic quantization and pruning.
			These methods reduced computational overhead and latency without
			significant accuracy loss, achieving a detection accuracy of 96.7%.
			
			
		\end{itemize}
	\end{frame}
	
	
		% Slide 1: Introduction / Problem Background
\begin{frame}{Introduction}
	\frametitle{Gunshot Detection ‚Äî A Need for Intelligent Safety Systems}
	\justifying
	Public safety systems increasingly rely on AI-driven solutions for real-time threat detection.  
	Gunshot detection is critical for:
	\begin{itemize}
		\item Rapid emergency response in urban and public spaces.
		\item Accurate identification of firearm-related acoustic events.
		\item Integration with surveillance networks for automated alert systems.
	\end{itemize}
	\vspace{0.2cm}
	\textbf{Challenge:} Traditional sound-based systems often fail in noisy or cluttered environments.
\end{frame}

%--------------------------------------------
% Slide 2: Motivation
\begin{frame}{Motivation}
	\frametitle{Why Gunshot Detection on Edge Devices?}
	\justifying
	Cloud-based gunshot detection systems face latency and privacy challenges.  
	To overcome these, lightweight models on embedded platforms like \textbf{Raspberry Pi} are ideal.
	\begin{itemize}
		\item Enables \textbf{real-time inference} close to the source.
		\item Reduces dependency on high-speed internet.
		\item Supports \textbf{scalable and low-power deployment}.
		\item Enhances community safety through rapid local alerts.
	\end{itemize}
	\vspace{0.3cm}
	\textbf{Goal:} Design an efficient and accurate AI-based gunshot detection model for edge deployment.
\end{frame}

%-----------------------------------------
\section{Base Paper Overview}
%-----------------------------------------
\begin{frame}{Base Paper Overview}
	\frametitle{Real-time Gunshot Detection System}
	\justifying
	The base paper proposes a real-time gunshot detection system for enhanced public safety. It captures environmental audio, extracts features (MFCC or YAMNet), and uses neural network models for accurate gunshot classification suitable for edge devices like Raspberry Pi.
	\begin{itemize}
		\item Audio capture and feature extraction
		\item ML-based gunshot classification (up to 96\% accuracy)
		\item Optimized for real-time, low-power hardware
	\end{itemize}
\end{frame}

%----------------------------------
\begin{frame}{Key Contributions and Limitations of Base Paper}
	\frametitle{Analysis of Base Paper}
	\justifying
	The base paper made significant contributions to real-time gunshot detection but also had limitations that inspired our enhancements:
	\begin{itemize}
		\item \textbf{Contributions:}
		\begin{itemize}
			\item Evaluation of multiple hardware platforms for real-time inference feasibility.
			\item Consideration of noisy environments and confounding audio events.
		\end{itemize}
		\item \textbf{Limitations:}
		\begin{itemize}
			\item Hardware optimization and low-latency inference were limited.
			\item Model compression techniques like pruning or quantization were not explored.
			\item Lightweight embedding-only inference for resource-constrained devices was not implemented.
			\item Model was trained on small dataset only leading to  overfitttig.
		\end{itemize}
	\end{itemize}
\end{frame}
%------------------------------------------------------
\begin{frame}{Base Paper Implementation}
	\frametitle{Gunshot Detection System Overview}
	\justifying
	The paper presents a real-time gunshot detection system integrated with camera surveillance. Audio data is pre-processed to improve model accuracy.
	\begin{itemize}
		\item Gunshot sounds from various firearms (AK-47, MP5, M16, etc.), resampled to 1-sec 22050 Hz and filtered for low-energy noise, yielding 3210 samples.
		\item Non-gunshot sounds (thunder, fireworks, drums, doors, clapping, barks) collected from YouTube, totaling 7758 samples after preprocessing.
	\end{itemize}
\end{frame}

%---------------------------------
\begin{frame}{Base Paper Implementation (Model Training)}
	\frametitle{Modeling Approaches}
	\justifying
	Two ML approaches were used for gunshot detection:
	\begin{enumerate}
		\item \textbf{YAMNet Transfer Learning:} 
		\begin{itemize}
			\item 1024-dim YAMNet embeddings as input.
			\item Three-layer network (512-unit dense + 2-output).
			\item 60/20/20 train/val/test split, 3200 samples/group.
			\item EarlyStopping, trained for 5 epochs.
		\end{itemize}
		\item \textbf{MFCC-Based LSTM:} 
		\begin{itemize}
			\item Audio ‚Üí MFCC features.
			\item LSTM: 128-unit LSTM ‚Üí Flatten ‚Üí Dense layers (128, 64) ‚Üí 9-output.
			\item 50 epochs, batch size 72, SparseCategoricalCrossentropy loss, EarlyStopping.
		\end{itemize}
	\end{enumerate}
\end{frame}
%---------------------------------------------
% --- Slide 1: YAMNet Transfer Learning Flow ---
\begin{frame}{YAMNet Transfer Learning Flow}
	\centering
	\includegraphics[width=0.9\textwidth]{yamnet_flow.png}
\end{frame}

% --- Slide 2: MFCC-Based LSTM Flow ---
\begin{frame}{MFCC-Based LSTM Flow}
	\centering
	\includegraphics[width=0.9\textwidth]{mfcc_lstm_flow.png}
\end{frame}

%----------------------------------------------
\begin{frame}{Base Paper Results}
	\frametitle{Gunshot Detection Model Performance}
	\justifying
	\textbf{A. TensorFlow YAMNet Transfer Learning Model:}
	\begin{itemize}
		
		\item Test set: Loss = 0.0490, Accuracy = 98.75\%
		\item Confusion Matrix (1280 samples): 
		\textbf{Gunshot: 637/640 correct}, \textbf{Non-gunshot: 635/640 correct}  
		\item Minimal misclassification demonstrates strong predictive power for real-world deployment.
	\end{itemize}
	
	\textbf{B. MFCC-Based LSTM Model:}
	\begin{itemize}
		
		\item Test set: Loss = 0.1676, Accuracy = 96.95\%
		\item Confusion Matrix (1280 samples): 
		\textbf{Gunshot: 623/640 correct}, \textbf{Non-gunshot: 618/640 correct}
		\item High accuracy indicates strong generalization for real-world use.
	\end{itemize}
\end{frame}
\section{Results Obtained}
%--------------------------------
\begin{frame}{Final Evaluation of Gunshot Detection Models}
	\frametitle{Test Performance Comparison}
	\justifying
	Both models achieved high accuracy on training, validation, and test sets. Final evaluation results are as follows:
	
	\begin{columns}[T, onlytextwidth]
		\column{0.48\textwidth}
		\textbf{YAMNet Transfer Learning}
		\begin{itemize}
			
			\item Test Loss: 0.0384
			\item Test Accuracy: 98.52\%
			\item Confusion Matrix:
			\[
			\begin{bmatrix}
				636 & 4 \\
				10 & 630
			\end{bmatrix}
			\]
		\end{itemize}
		
		\column{0.48\textwidth}
		\textbf{MFCC-Based LSTM}
		\begin{itemize}
			\item Test Loss: 0.1706
			\item Test Accuracy: 96.04\%
			\item Confusion Matrix:
			\[
			\begin{bmatrix}
				619 & 21 \\
				23 & 617
			\end{bmatrix}
			\]
		\end{itemize}
	\end{columns}
\end{frame}
%--------------------------------------------------
\begin{frame}{MFCC vs YAMNet Feature Matrices}
	\begin{columns}
		% Left column: MFCC
		\column{0.5\textwidth}
		\centering
		\textbf{MFCC\_Matrix}
		\vspace{0.2cm}
		
		\includegraphics[width=\textwidth]{MFCC_Matrix.png}
		\vspace{0.2cm}
		
		\small Spectral‚Äìtemporal representation extracted using MFCC features.
		
		% Right column: YAMNet
		\column{0.5\textwidth}
		\centering
		\textbf{YAMNet\_Matrix}
		\vspace{0.2cm}
		
		\includegraphics[width=\textwidth]{YAMNet_Matrix.png}
		\vspace{0.2cm}
		
		\small Deep audio embeddings learned by YAMNet pretrained model.
	\end{columns}
	
	\vspace{0.4cm}
	\justifying
	\small
	The MFCC matrix captures handcrafted frequency features, while YAMNet embeddings
	provide high-level semantic representations learned from large-scale audio datasets.
\end{frame}
%--------------------------------
%--------------------------------
\begin{frame}{Model Performance Comparison}
	\frametitle{YAMNet vs MFCC-LSTM Evaluation Summary}
	\centering
	\vspace{-0.3cm}
	\scriptsize % makes the table fit nicely on one slide
	\renewcommand{\arraystretch}{1.25}
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Metric} & \textbf{YAMNet Transfer Learning} & \textbf{MFCC-Based LSTM} \\
		\hline
		Test Loss & 0.0384 & 0.1706 \\
		\hline
		Test Accuracy & 98.52\% & 96.04\% \\
		\hline
		Gunshot Correct (out of 640) & 636 & 619 \\
		\hline
		Non-Gunshot Correct (out of 640) & 630 & 617 \\
		\hline
		Total Misclassifications & 14 & 44 \\
		\hline
		Inference Complexity & Low (Dense only) & Moderate (LSTM + Dense) \\
		\hline
		FPGA Deployment Suitability & \textbf{High} & Medium \\
		\hline
	\end{tabular}
	
	\vspace{0.3cm}
	\normalsize
	\textbf{Conclusion:} YAMNet-based model achieves higher accuracy and efficiency for real-time deployment.
\end{frame}


%-----------------------------------------------
%--------------------------------
%--------------------------------



%--------------------------------
%--------------------------------
\begin{frame}{Training Accuracy and Loss ‚Äî MFCC + LSTM Model}
	\frametitle{MFCC + LSTM Model Performance}
	\justifying
	The MFCC-LSTM model achieved strong accuracy, though with slightly higher loss and slower convergence.
	\vspace{0.3cm}
	
	\centering
	\textbf{Accuracy and Loss Curves} \\
	\includegraphics[width=0.8\textwidth]{accuracy_curve_MFCC.png}
	
	\vspace{0.3cm}
	\textbf{Final Test Accuracy:} 96.04\% \quad | \quad \textbf{Test Loss:} 0.1706
\end{frame}


%----------------------------------------------------------
%-------------------------------------------------


%-----------------------------------------------
%--------------------------------------------
	\section{Model Advancements}
	%-----------------------------------------------
\begin{frame}{Proposed Advanced Model ‚Äî CNN14 + BiLSTM + Attention}
	\justifying
	\begin{columns}[T,onlytextwidth]
		
		% ===== Left Column =====
		\begin{column}{0.58\textwidth}
			\vspace{-0.2cm}
			\textbf{Goal:} Improve gunshot detection by combining spectral‚Äìtemporal learning and attention focus.
			
			\vspace{0.2cm}
			\textbf{Model Highlights:}
			\begin{itemize}
				\item \textbf{CNN14 (PANNs):} Extracts deep log-Mel embeddings from preprocessed audio.
				\item \textbf{BiLSTM:} Learns forward‚Äìbackward temporal context of gunshot events.
				\item \textbf{Attention:} Weighs key frames, suppresses noise/silence.
				\item \textbf{Dense Head:} 128‚Äì64 ReLU units + dropout.
				\item \textbf{Output:} Softmax ‚Üí \textit{Gunshot / Non-Gunshot}.
			\end{itemize}
		\end{column}
		
		% ===== Right Column =====
		\begin{column}{0.4\textwidth}
			\centering
			\includegraphics[width=\linewidth,height=4.6cm,keepaspectratio]{architecture_model.png}\\
			{\scriptsize CNN14 ‚Üí BiLSTM ‚Üí Attention ‚Üí Dense ‚Üí Output}
		\end{column}
		
	\end{columns}
\end{frame}




	%----------------------------------------------------------
\begin{frame}{Dataset Preparation}
	\justifying
	To develop a robust and generalized gunshot detection system, audio samples were collected and organized from multiple open-source repositories.  
	The datasets were categorized into two primary classes ‚Äî \textbf{Gunshot} and \textbf{Non-Gunshot} ‚Äî to ensure balanced binary classification.
	
	\vspace{0.3cm}
	\textbf{Datasets Used:}
	
	\begin{columns}[T,onlytextwidth]
		\begin{column}{0.48\textwidth}
			\textbf{Gunshot Audio Sources:}
			\begin{itemize}
				\item Gunshot Audio Dataset (Kaggle)
				\item Mendeley Gunshot Dataset
				\item Gunshot/Gunfire Dataset (Zenodo ‚Äì Edge Collected)
				\item MAD ‚Äì Military Audio Dataset
			\end{itemize}
		\end{column}
		
		\begin{column}{0.48\textwidth}
			\textbf{Non-Gunshot Audio Sources:}
			\begin{itemize}
				\item UrbanSound8K Dataset
				\item ESC-50 Environmental Sound Dataset
			\end{itemize}
		\end{column}
	\end{columns}
	
	\vspace{0.3cm}
	\textbf{Dataset Composition:}
	\begin{itemize}
		\item Total of \textbf{17,746 audio clips}, with \textbf{8,873 samples per class}.
		\item Ensured balanced representation of diverse real-world gunshot and background sounds.
	\end{itemize}
\end{frame}

%-----------------------------------------------
\begin{frame}{Data Preprocessing Pipeline}
	\begin{columns}[T,onlytextwidth]
		
		% ========== LEFT COLUMN ==========
		\begin{column}{0.52\textwidth}
			\textbf{1. Raw Data Filtering}
			\begin{itemize}
				\item Loaded all audio clips at \textbf{16 kHz, mono}.
				\item Applied \textbf{RMS power filtering} to remove silent or low-energy files:
				\begin{itemize}
					\item Threshold: \textbf{RMS} $\geq$ 0.002
					\item At least one RMS peak (\textbf{min\_peak\_count} = 1)
				\end{itemize}
				\item Result: kept \textbf{5,773 / 8,883} valid gunshot clips.
			\end{itemize}
			
			\vspace{0.1cm}
			\textbf{2. Balancing and Sliding Windows}
			\begin{itemize}
				\item Balanced dataset by selecting \textbf{5,773 non-gunshot} samples
				from UrbanSound8K \& ESC-50 categories.
				\item Generated fixed-length \textbf{sliding windows (‚âà1s)} from both classes.
			\end{itemize}
		\end{column}
		
		% ========== RIGHT COLUMN ==========
		\begin{column}{0.46\textwidth}
			\textbf{3. Feature Extraction (Librosa)}
			\begin{itemize}
				\item Sampling rate: \textbf{32 kHz}
				\item Computed \textbf{log-Mel spectrograms}:
				\begin{itemize}
					\item 64 Mel bins, 10 ms hop, 50‚Äì14 kHz band
					\item Per-frequency normalization (zero mean, unit variance)
				\end{itemize}
			\end{itemize}
			
			\vspace{0.15cm}
			\centering
			\includegraphics[width=4cm,height=4cm,keepaspectratio]{Pre_processing_Adv.png}
		\end{column}
		
	\end{columns}
\end{frame}
%=---------------------------------------------
\begin{frame}{Final Model ‚Äî CNN14 Embedding Classifier}
	\justifying
	A lightweight \textbf{CNN14 embedding-based classifier} designed for real-time gunshot detection. Redundant audio layers removed while preserving key learned representations.
	
	\vspace{0.2cm}
	\textbf{Architecture:}
	\begin{itemize}
		\item \textbf{Input:} Log-Mel spectrogram (1√ó64√ó400), 4 s audio.
		\item \textbf{Backbone:} CNN14 encoder (PANNs) up to global avg pooling.
		\item \textbf{Embedding:} 2048-D feature vector.
		\item \textbf{Head:} Dense(512, ReLU) ‚Üí Dropout(0.3) ‚Üí Dense(1, Sigmoid).
	\end{itemize}
	
	\vspace{0.15cm}
	\textbf{Training:}
	\begin{itemize}
		\item Loss: Binary Crossentropy, Optimizer: Adam (lr = 1e-4).
		\item Metrics: Accuracy, AUC; 50 epochs.
		\item Backbone frozen, then fine-tuned.
	\end{itemize}
	
	\vspace{0.2cm}
	\begin{columns}[T,onlytextwidth]

		
		\begin{column}{0.45\textwidth}
			\small
			\textbf{Highlights:}
			\begin{itemize}
				\item Embedding-only ‚Üí fewer ops & memory.
				\item Deployable on \textbf{Raspberry Pi}.
				\item High accuracy, low latency.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

	%-----------------------------------------------
	
	\section{Result}}
	%-----------------------------------------------
\begin{frame}{Model Evaluation Overview \& Real-Time Superiority}
	\justifying
	\textbf{Evaluation Summary:}
	\begin{itemize}
		\item Developed and trained a CNN14 + BiLSTM + Attention model on 4s log-Mel spectrogram inputs.
		\item Dataset split: 60\% Training, 20\% Validation, 20\% Testing.
		\item Optimizer: \textbf{Adam (lr = 1e-4)}; Loss: \textbf{CrossEntropyLoss}; Total epochs: 50.
	\end{itemize}
	
	\vspace{0.25cm}
	\textbf{Why This Model Excels in Real-Time:}
	\begin{itemize}
		\item \textbf{Embedding-only Design:} Uses pre-trained CNN14 embeddings, eliminating redundant convolutional layers for faster computation.
		\item \textbf{Lightweight Inference:} Compact structure with minimal parameters enables deployment on low-power devices like \textbf{Raspberry Pi}.
		\item \textbf{BiLSTM-Attention Mechanism:} Learns key temporal and spectral cues, allowing rapid detection of impulsive sounds amidst noise.
		\item \textbf{Optimized Trade-off:} Achieves high accuracy (\textasciitilde97.6\%) with reduced latency and memory footprint.
		\item \textbf{Edge-Ready Deployment:} Ensures stable performance in real-world, noisy outdoor conditions.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Outcome:} A robust and efficient gunshot detection framework achieving near real-time response without compromising precision.
\end{frame}

	%----------------------------------------------
\begin{frame}{Performance Results and Analysis}
	\justifying
	\textbf{Training Summary:}
	\begin{itemize}
		\item \textbf{Epochs:} 50 \hspace{0.5cm} \textbf{Optimizer:} Adam (lr = 0.001)
		\item \textbf{Loss Function:} CrossEntropyLoss()
		\item \textbf{Best Validation Accuracy:} \textbf{97.57\%}
	\end{itemize}
	
	\vspace{0.15cm}
	\textbf{Key Observations:}
	\begin{itemize}
		\item Rapid convergence with minimal overfitting (EarlyStopping + Dropout).
		\item Attention layer improves focus on gunshot frames, enhancing noise resilience.
		\item Stable performance across urban and open-field tests.
	\end{itemize}
	
	\vspace{0.2cm}
	\begin{columns}[T,onlytextwidth]
		\begin{column}{0.48\textwidth}
			\centering
			\includegraphics[width=\linewidth,height=3.2cm,keepaspectratio]{acc vs loss_adv.png}\\
			{\scriptsize Accuracy vs Loss Accuracy}
		\end{column}
		\begin{column}{0.48\textwidth}
			\centering
			\includegraphics[width=\linewidth,height=3.2cm,keepaspectratio]{confusion_matrix.png}\\
			{\scriptsize Confusion Matrix (Gunshot vs Non-Gunshot)}
		\end{column}
	\end{columns}
	
	\vspace{0.15cm}
	\centering
\end{frame}
%-------------------------------------
\section{Implementation}
\begin{frame}{Raspberry Pi Implementation ‚Äî System Setup}
	\justifying
	\textbf{Hardware Setup:}
	\begin{itemize}
		\item \textbf{Raspberry Pi 4} (4GB RAM) for edge deployment.
		\item USB microphone or external audio input for real-time audio capture.
		\item Optional: small speaker/alert system for local notifications.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Software Stack:}
	\begin{itemize}
		\item Python 3.x environment with \textbf{Librosa, Numpy, Torch}.
		\item Pre-trained \textbf{CNN14 embedding extractor} and BiLSTM-Attention classifier converted for lightweight edge inference.
		\item Optimized audio pipeline:
		\begin{itemize}
			\item Capture 1-second audio segments.
			\item Convert to log-Mel spectrogram.
			\item Pass through embedding extractor ‚Üí BiLSTM-Attention classifier.
		\end{itemize}
	\end{itemize}

	\textbf{Workflow Overview:}
	\begin{center}
		\includegraphics[width=0.85\linewidth,height=3cm,keepaspectratio]{Raspberry Pi Workflow.png} \\
		{\scriptsize Audio Capture ‚Üí Preprocessing ‚Üí Embedding ‚Üí Classification ‚Üí Alert}
	\end{center}
\end{frame}
%------------------------------------------------
\begin{frame}{Raspberry Pi Implementation ‚Äî Real-Time Inference}
	\justifying
	\textbf{Inference Pipeline:}
	\begin{itemize}
		\item Continuous audio stream segmented into 1-second windows.
		\item Preprocessing and log-Mel extraction applied in real-time.
		\item CNN14 embedding extracted, followed by BiLSTM-Attention classification.
		\item Softmax output triggers \textbf{real-time alerts} for gunshot detection.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Optimizations for Edge Deployment:}
	\begin{itemize}
		\item Reduced model size using \textbf{embedding-only design}.
		\item Minimal CPU/memory usage for stable performance on Raspberry Pi.
		\item Low-latency inference (~100‚Äì200ms per segment).
		\item Robust to environmental noise, tested in urban and open-field scenarios.
	\end{itemize}
	
	\vspace{0.2cm}
	\centering
	\textbf{Outcome:}  
	\textit{A fully functional, low-latency, real-time gunshot detection system deployable on edge devices.}
\end{frame}


%--------------------------------
	\section{Comparison With Base Paper}}

	%-------------------------------------------
\begin{frame}{Baseline vs Proposed Model ‚Äî Conceptual Comparison}
	\justifying
	\begin{columns}[T,onlytextwidth]
		% Baseline Paper
		\begin{column}{0.48\textwidth}
			\textbf{Baseline (MFCC + LSTM Paper):}
			\begin{itemize}
				\item Audio converted to MFCCs (FFT: 512, Hop: 255)
				\item Single LSTM (128 units) + Flatten + Dense(128,64) + Dropout
				\item Output: Softmax (9 classes in original; binary subset used here)
				\item Preprocessing: Resampled 1s, 22050 Hz, sliding-window 2000 Hz
				\item Gunshot power filter applied; non-gunshot not thresholded
				\item Training: SparseCategoricalCrossentropy, Adam, 50 epochs, batch 72
			\end{itemize}
			\centering

		\end{column}
		
		% Proposed Model
		\begin{column}{0.48\textwidth}
			\textbf{Proposed Model (CNN14 + BiLSTM + Attention):}
			\begin{itemize}
				\item Embedding-only CNN14 extracts high-level log-Mel features
				\item BiLSTM captures bidirectional temporal patterns
				\item Attention highlights key frames, suppressing silent/noisy regions
				\item Dense layers (512 ‚Üí 1) with Dropout, output: Sigmoid (binary)
				\item Preprocessing: Sliding-window 1s segments, RMS power filter, balanced classes
				\item Optimized for edge devices; faster inference & low memory
			\end{itemize}
			\centering

		\end{column}
	\end{columns}
\end{frame}

	%--------------------------------------------
\begin{frame}{Performance Comparison ‚Äî Baseline vs Proposed}
	\justifying
	\centering
	\small
	\begin{tabular}{l c c}
		\hline
		\textbf{Metric} & \textbf{Baseline} & \textbf{Proposed} \\
		\hline
		Gunshot Samples & 3,210 & 5,773 \\
		Non-Gunshot Samples & 3,600 & 5,773 \\
		Input & MFCC (128√óN) & Log-Mel (64√ó400) \\
		Temporal Model & LSTM & BiLSTM + Attention \\
		Validation Accuracy & ~96.95\% & 97.57\% \\
		Inference Latency & High & Low (Edge-ready) \\
		Model Size & Medium & Compact \\
		Noise Robustness & Moderate & High \\
		Deployment & Raspberry Pi & Raspberry Pi \\
		\hline
	\end{tabular}
	
	\vspace{0.3cm}
	\centering
	{\scriptsize Proposed model improves accuracy, reduces latency, and is robust for real-time edge deployment.}
	
	\vspace{0.2cm}
	\begin{columns}[c,onlytextwidth]
		\begin{column}{0.33\textwidth}
			\centering
			\textbf{‚úÖ Higher Accuracy} \\
			97.57\% validation
		\end{column}
		\begin{column}{0.33\textwidth}
			\centering
			\textbf{‚ö° Low Latency} \\
			Edge-ready for Raspberry Pi
		\end{column}
		\begin{column}{0.33\textwidth}
			\centering
			\textbf{üéß Robust} \\
			Performs well in noisy conditions
		\end{column}
	\end{columns}
\end{frame}
%-------------------------------------------------
\begin{frame}{Conclusion ‚Äî Methodology}
	\justifying
	\textbf{Dataset and Preprocessing:}
	\begin{itemize}
		\item Curated a balanced dataset of \textbf{17,746 audio clips} (8,873 gunshot, 8,873 non-gunshot).
		\item Applied \textbf{RMS power filtering} to remove silent/irrelevant clips.
		\item Converted all audio to \textbf{1-second mono segments} and applied \textbf{sliding-window segmentation}.
		\item Extracted \textbf{log-Mel spectrograms} and embeddings from pre-trained CNN14 for transfer learning.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Model Development:}
	\begin{itemize}
		\item Built an \textbf{embedding-only CNN14 + BiLSTM + Attention} model for gunshot detection.
		\item Dense layers with dropout ensure generalization; output layer predicts binary classes (\textit{Gunshot / Non-Gunshot}).
		\item Optimized for \textbf{real-time inference} on Raspberry Pi / FPGA.
	\end{itemize}
\end{frame}
%-------------------------
\begin{frame}{Conclusion ‚Äî Results and Key Takeaways}
	\justifying
	\textbf{Performance Highlights:}
	\begin{itemize}
		\item Achieved \textbf{97.57\% validation accuracy}, outperforming baseline MFCC + LSTM (~96\%).
		\item Low inference latency and compact model size for edge deployment.
		\item Attention mechanism improves focus on key frames and robustness to noise.
		\item Balanced training dataset ensures consistent performance across environments.
	\end{itemize}
	
	\vspace{0.2cm}
	\textbf{Key Takeaway:}
	\begin{itemize}
		\item Our approach demonstrates a \textbf{robust, real-time, and accurate gunshot detection system}.
		\item Outperforms classical MFCC + LSTM methods in both accuracy and edge deployment feasibility.
		\item Complete end-to-end workflow: preprocessing ‚Üí embedding ‚Üí model training ‚Üí evaluation ‚Üí real-time inference.
	\end{itemize}
\end{frame}

%-----------------------------------------------
% REFERENCES SLIDE ADDED
%-----------------------------------------------
\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{References}
	\justifying
	\small
	\begin{thebibliography}{99}
		
		\bibitem{Xiong2023}
		Xinzhang Xiong,  
		\href{run : 20230916_GunshotDetection_XIONG_XINZHANG.pdf}%
		{‚ÄúReal-time Gunshot Detection System Integration to Camera Surveillance System‚Äù},  
		Pennsylvania State University,  
		State College, USA.  
		Email: \href{mailto:xpx5059@psu.edu}{xpx5059@psu.edu}
		
		\bibitem{YAMNet}
		Google Research,  
		\textit{‚ÄúYAMNet: Pretrained Audio Event Classification Network‚Äù},  
		Available at: \url{https://github.com/tensorflow/models/tree/master/research/audioset/yamnet}
		
		\bibitem{PANNs}
		Kong, Qiuqiang, et al.  
		\textit{‚ÄúPANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition‚Äù},  
		IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020.
		
		\bibitem{MFCC}
		T. Giannakopoulos,  
		\textit{‚ÄúAudio Feature Extraction using MFCCs for Environmental Sound Classification‚Äù},  
		Elsevier, 2015.
		
		\bibitem{EdgeAI}
		S. Han, H. Mao, and W. J. Dally,  
		\textit{‚ÄúDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding‚Äù},  
		ICLR, 2016.
		
	\end{thebibliography}
\end{frame}


	%-----------------------------------------------

	
	\section*{Acknowledgement}  
	\begin{frame}
		\textcolor{myNewColorA}{\Huge{\centerline{Thank you!}}}
	\end{frame}
	
\end{document}